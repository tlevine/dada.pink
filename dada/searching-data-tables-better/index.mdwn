You may have noticed that I've been much less active for the past few months.
This is partly because I've been acquiring some new datasets and setting up some
new infrastructure, and that involves some up-front work. Also, I've been
thinking a bit about how I might find spreadsheets that are related to each
other, and I'm going to tell you about that below.

## Preface: Open data is good for business!?
But before I get to the main part of this article, I'm going to rant about the
other thing that has kept me busy.

I've been doing these silly open data studies for a while for fun,
and people have been saying that they're informative and useful and
important and whatnot, but people don't want to (or can't) pay me for
the open data stuff. So I gave up at finding funding for my silly
open data studies and over the past few months am doing some totally
unrelated work for money instead. I've also very recently received a
small bit of funding from some people to do silly open data stuff.
That's very nice, but it's small.

I understand that life isn't all about money, but sometimes it seems like
open data **is** all about money; that's pretty much
[all](http://beyondtransparency.org/chapters/preface/)
[I](https://theodi.org/guides/how-make-business-case-open-data)
[ever](http://www.socrata.com/blog/economic-impact-open-data/)
[hear](http://www.forbes.com/sites/bethsimonenoveck/2014/01/08/from-faith-based-to-evidence-based-the-open-data-500-and-understanding-how-open-data-helps-the-american-economy/)
about why you need open data.
Since open data is supposed to be good for business,
it annoys me that people don't want to pay for it.

Could it be that open data isn't good for business?
Or maybe it is good for business but only because it is
externalizing business costs to volunteers.

Here's what I think.
Open data [isn't all about money](http://sunlightfoundation.com/blog/2014/04/22/beyond-business-the-impacts-of-open-data/),
and the [business value isn't particularly compelling](http://blog.okfn.org/2013/11/08/open-datas-business-value-isnt-that-important/).
But things are only true if you can make money from them,
so open data is only good if you can make money from it.
Thus, everyone keeps
saying that open data is going to save money, enable new businesses,
and create new jobs. They're also saying that it's also going to end
corruption, make people trust governments, and otherwise save the world.
Open data is probably going to do some good, but these sorts of assertions
are exaggerated.

All of this hype is really confusing for me, and listening to it is a lot of
work. I wish we could make less of a big deal out of the little open data
toys that we're all making. Also, I still don't know what open data is.

## Abrupt transition
Now that I'm finished complaining about open data hype and lack of funds,
I'm going tell you about searching datasets.

## People don't know what's in all these open data
For the past year I've been looking at lots of datasets from lots of open
data catalogs. People seem interested in my stupidly basic analyses across
all of these datasets, and that tells me that people don't know what's in
these datasets.

### Too many datasets for manual review
These "datasets" are usually spreadsheets;
how do we figure out what's in a bunch of spreadsheets today?

![A spreadsheet](/dada/pagerank-for-spreadsheets/bus-spreadsheet.png)

We could manually look at every cell in every spreadsheet,
but this would take a long time.

### Data publishing guidelines
It might take a bit less long if we have good spreadsheets.
so people make guidelines as to how a person or organization
with lots of datasets might allow other people to use them.
Here are some guidelines.

* Open Knowledge Foundation [Open Data Census](http://census.okfn.org/)
* Tim Berners-Lee [Five Stars](http://inkdroid.org/journal/2010/06/04/the-5-stars-of-open-linked-data/) of open linked data.
* Open Government Working Group [8 Principles of Open Government Data](http://www.opengovdata.org/home/8principles)
* Sunlight Foundation [Open Data Policy Guidelines](http://sunlightfoundation.com/opendataguidelines/)
* Open Data Institute [Certificates](https://certificates.theodi.org/)

At a basic level, many guidelines suggest that data
be available on the internet and under a free license; at the other
end of the spectrum, guidelines suggest that data be in standard
formats accompanied with assorted metadata.

These guidelines put
responsibility on the publishers of data to do certain things when
they publish data. This can be a lot of work, so it's unreasonable
for us to expect this to happen for anything other than the most
important of datasets.

### Standard Schemas

Many groups have developed standard schemas for particular sorts
of data. Here are some examples.

* RSS, Atom, &c.
* [GeoJson](http://geojson.org/)
* LIVES ([Yelp](http://www.yelp.com/healthscores), [Code for America](http://foodinspectiondata.us/))

When I say "schema", I loosly mean "a definition of
what the columns should be and what the rows mean".

![](/dada/pagerank-for-spreadsheets/hovering-schema.png)

For example, I'll tell you a bit about the schema of this spreadsheet.
I asked a bunch of people how they use toilets in differenc scenarios,
and some of the results are in this spreadsheet. I asked each person
about twelve different scenarios. Each row in the spreadsheet corresponds
to a scenario within a person; there is one row per person.
The "participantId" column defines the person, and the "task", "privacy",
and "cleanliness" columns define the scenario; these four columns thus
act as a unique index on the table. The other columns are about their
responses to the questions. I'll save the full discussion for a talk
about the [viscious hovering cycle](/dada/hovering-cycle); the point here
is that a schema is roughly a definition of what the different columns
and rows mean and what values are allowed in the cells.

As you probably know from experience,
this is helpful when different organizations have similar datasets.
For example, this can work if multiple bloggers are publishing blog
posts or if multiple cities are publishing restaurant inspection results.
It still puts a burden on the publishers of data.

### Graphs and stuff

Lots of people say that graphs make us understand data. So a lot of these
repositories of spreadsheets let people make graphs.

On the French national rail company data site, you can look at tables,

![](/dada/pagerank-for-spreadsheets/sncf-tableau.png)

but you can also make graphs!

![](/dada/pagerank-for-spreadsheets/sncf-analyse.png)

OpenData Lombardia does it too!

![](/dada/pagerank-for-spreadsheets/lombardia-grafico.png)

These graphs are nice, but we still have the same problem as before:
Even if we have the most perfect graphs about each separate spreadsheet,
we won't have time to look at of the millions of spreadsheets; before
it's worth looking at these graphs, we have to choose a small group of
spreadsheets to make graphs about.

### What usually happens: Word of mouth and other informal means

In practice, I find that people learn what's in a spreadsheet through word
of mouth, without looking directly at the spreadsheet. I know what's in the
various datasets published by the United States Census, but I've never used
any of them myself. I know what's in New York City's tax parcel boundaries
dataset (PLUTO) because lots of people have been
[talking](http://spatialityblog.com/2013/04/04/a-modest-proposal-for-nyc-tax-parcel-data/)
[about](http://www.codeforamerica.org/blog/2013/07/25/epic-win-for-nycs-open-data-community-pluto-is-free/)
it, but I haven't looked at it myself.
And I know that several cities release data about the use of bike-share programs
because people get excited every time this happens, but again, I haven't looked
at any of it myself.

Sometimes people get fancy and make
[lists of data sources](https://github.com/amandabee/cunyjdata/wiki/Where-to-Find-Data),
and I group this in the same category; people are finding out about datasets through
references rather than by looking at the datasets.

The people I meet who know the most about what's in these large collections
of disparate open data spreadsheets are the people in charge of putting them
on the internet as part of an open data initiative. These people usually work
either for the government (or other big organization) providing the data or the
vendor providing the open data catalog to the government. For any one
organization's data, there are at most a couple of people who have a good idea
about what data the organization has.

Surprisingly to me,
these often aren't the ones who are looking very analytically at the spreadsheets.
Their jobs is to talk to get other people to put data on the data catalog; they're
not running experiments or making models or otherwise studying the
data substantially.
And the open data initative is probably just a small part of their job.
If you want to know whether a city
publishes a particular dataset, these are the people you want to ask.

Unfortunately, these people don't have time to serve as everyone's personal
search engine, and there isn't an easy way for them to convey all of this
knowledge to others. Moreover, they only know about the data they've worked
on in their respective governments; they don't know much about other governments.

## Why you might have trouble searching for spreadsheets
I see [two main issues](/dada/searching-data-tables)
in the common means of searching through spreadsheets.
The first issue is that the search method is quite naive; these websites are
usually running keyword searches.
The second issue is that the search is localized to datasets that are published
or otherwise managed by a particular entity; it's hard to search for
spreadsheets without first identifying a specific publisher or repository.

You can search in a generic web search engine, but the spreadsheets might not
be indexed, and the search might not work that well even if the spreadsheets
are indexed.

You can use the search bars on these various websites, but they usually do a
very naive keyword search. Also, these sites tend to be quite slow.

### Dabbling with other search paradigms
Having assembled a spreadsheet about my spreadsheets, I was quite able to
search through them. Even though I only had metadata in this spreadsheet,
I could do powerful searches because I could create any metadata I wanted.
Here's an example. In my composition of music from spreadsheets and in my
teaching, I often want a dataset whose schema has particular characteristics.
For example, I might want a dataset with lots of date columns,
so I collected data about the column types in these various tables and ran
queries like this on my spreadsheet.

    SELECT * FROM datasets WHERE "ncol.date" > 3;

I can also look for datasets with the exact same column names. For example,
all of the following datasets have the same column names.

* [Math Test Results 2006-2012 - Citywide - Gender](https://data.cityofnewyork.us/d/2bh6-qmgg)
* [Math Test Results 2006-2012 - Citywide - Ethnicity](https://data.cityofnewyork.us/d/vve2-26rs)
* [English Language Arts (ELA) Test Results 2006-2012 - Citywide - SWD](https://data.cityofnewyork.us/d/d72n-ivax)
* [English Language Arts (ELA) Test Results 2006-2012 - Citywide - ELL](https://data.cityofnewyork.us/d/72db-huua)
* [Math Test Results 2006-2012 - Citywide - SWD](https://data.cityofnewyork.us/d/ufu7-zp25)
* [English Language Arts (ELA) Test Results 2006-2012 - Citywide - All Students](https://data.cityofnewyork.us/d/89di-hi4s)
* [Math Test Results 2006-2012 - Citywide - ELL](https://data.cityofnewyork.us/d/ngbi-cq85)
* [English Language Arts (ELA) Test Results 2006-2012 - Citywide - Gender](https://data.cityofnewyork.us/d/cs9m-cz6f)
* [Math Test Results 2006-2012 - Citywide - All Students](https://data.cityofnewyork.us/d/fxwm-3t4n)
* [English Language Arts (ELA) Test Results 2006-2012 - Citywide - Ethnicity](https://data.cityofnewyork.us/d/p5w7-g72z)

And here are the columns, in case you're curious.

* "grade"
* "year"
* "demographic"
* "number_tested"
* "mean_scale_score"
* "num_level_1"
* "pct_level_1"
* "num_level_2"
* "pct_level_2"
* "num_level_3"
* "pct_level_3"
* "num_level_4"
* "pct_level_4"
* "num_level_3_and_4"
* "pct_level_3_and_4"

These datasets are clearly related, so searching for datasets based on schema
might not be a horrible idea.


### Dealing with website-specific searches
![](/dada/data-about-open-data-talk-december-2-2013/unsilo.jpg)

When I'm looking for spreadsheets, the publishing organization is unlikely
to be my main concern. For example, if I'm interested in data about the
composition of different pesticides, but I don't really care whether the
data were collected by this city government or by that country government.

In my view, we can have one set of tools/people that focus on making
data available in a crude form and another set for assembling these crude
data into something more relevent to the people who want the data

And that's why I made
[OpenPrism](http://openprism.thomaslevine.com).
This is a disgustingly simple site that
forwards your search query to 100 other sites that house spreadsheets.

## A search engine for spreadsheets!?
I just articulated two issues with searching spreadsheets.
I started to address the issue of search paradigm by pulling schema-related
features out of the spreadsheets, and I started to address the issue of
site-specific search by letting
people search many sites at once. Taking this further, I've been thinking
about what it would mean to have a search engine for spreadsheets.

When we search for ordinary written documents, we send words into a search
engine and get pages of words back.

![](/dada/pagerank-for-spreadsheetst/wordsearch.png)

What if we could search for spreadsheets
by sending spreadsheets into a search engine and getting spreadsheets back?
The order of the results would be determined by various specialized statistics;
just as we use PageRank to find relevant hypertext documents, we can develop
other statistics that help us find relevant spreadsheets.

![](/dada/pagerank-for-spreadsheetst/commasearch.png)

So now I'm looking for ways to do interesting searches on spreadsheets.

I think a lot about
[rows and columns](http://www.datakind.org/blog/whats-in-a-table/).
When we define tables in relational
databases, we can say reasonably well what each column means, based on
names and types, and what a row means, based on unique indices.
In spreadsheets, we still have column names, but we don't get everything
else.

The unique indices tell us quite a lot; they give us an idea about the
observational unit of the table and what other tables we can nicely
join or union with that table. So I made a package that finds unique
indices in ordinary CSV files.

    pip3 install special_snowflake

It's called "special snowflake", but it needs a better name.

If we pass the iris dataset to it, ::

    "Sepal.Length","Sepal.Width","Petal.Length","Petal.Width","Species"
    5.1,3.5,1.4,0.2,"setosa"
    4.9,3,1.4,0.2,"setosa"
    4.7,3.2,1.3,0.2,"setosa"
    4.6,3.1,1.5,0.2,"setosa"
    ...

we get no unique keys.

    >>> special_snowflake.fromcsv(open('iris.csv'))
    set()

because no combination of columns uniquely identifies the rows.
Of course, if we add an identifier column,

    "Id","Sepal.Length","Sepal.Width","Petal.Length","Petal.Width","Species"
    1,5.1,3.5,1.4,0.2,"setosa"
    2,4.9,3,1.4,0.2,"setosa"
    3,4.7,3.2,1.3,0.2,"setosa"
    4,4.6,3.1,1.5,0.2,"setosa"
    ...

that one gets returned.

    >>> special_snowflake.fromcsv(open('iris.csv'))
    {('Id',)}

For a more interesting example, let's look at chickweight.

    "weight","Time","Chick","Diet"
    42,0,"1","1"
    51,2,"1","1"
    59,4,"1","1"
    64,6,"1","1"
    76,8,"1","1"
    ...

I could read the documentation on this dataset and tell you
what its statistical unit is (`?ChickWeight` in R), or I could
just let `special_snowflake` figure it out for me.

    >>> special_snowflake.fromcsv(open('chick.csv'))
    {('Time', 'Chick')}

The statistical unit is chicks in time. That is, something was
observed across multiple chick, and multiple observations were
taken from each (well, at least one) chick.

Some spreadsheets are have less obvious identifiers. In this
table of 1219 rows and 33 columns,

    >>> from urllib.request import urlopen
    >>> url = 'http://data.iledefrance.fr/explore/dataset/liste-des-points-de-contact-du-reseau-postal-dile-de-france/download/?format=csv'
    >>> fp = urlopen(url)
    >>> special_snowflake.fromcsv(fp, delimiter = ';')
    {('adresse', 'code_postal'),
     ('adresse', 'localite'),
     ('identifiant',),
     ('libelle_du_site',),
     ('wgs84',)}

we find five functional unique keys. Just by looking at the column names,
I'm gussing that the first two are combinations of parts of the postal address
and that the latter three look are formal identifiers.
And when I do things correctly and look at the
[data dictionary](http://data.iledefrance.fr/api/datasets/1.0/liste-des-points-de-contact-du-reseau-postal-dile-de-france/attachments/laposte_description_champs_pointdecontact_pdf/),
I come to the same interpretation.

This tells me that this dataset is about postal service locations,
with one location per row. It also gives me some ideas as to things that can
act as unique identifiers for postal service locations.

It's kind of cool to run this on individual spreadsheets, but it's even cooler
to run this on lots of spreadsheets.
In [blizzard](https://pypi.python.org/pypi/blizzard), I find spreadsheets with
the same unique indices, and then I look for overlap between those spreadsheets.

![](/dada/pagerank-for-spreadsheets/network.png)

Spreadsheets with high overlap might be good to join to each other, and
spreadsheets with low overlap might be good to union with each other.
All of this is quite crude at the moment, so I'm somewhat surprised that
anything interesting comes out.

## Anti-climax
I don't yet have a nice package for searching spreadsheets, so I'll just
leave you with the following thought:
The search engine for spreadsheets is going to look a bit different from
the search engine for words.
