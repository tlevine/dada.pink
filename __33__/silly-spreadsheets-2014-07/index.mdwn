[[!meta title="Tom does silly things with lots of spreadshets"]]

*These are materials for [my talk at SEMS](http://spreadsheetlab.org/2014/04/10/sems-accepted-papers-published/). Blockquotes, code blocks, and images will be slides projected for everyone to see, and everything else is things that I'll say. Build them with `make slides.html`.*

> Tom does silly things with lots of spreadsheets.
> =================================================

I forgot what the title of this presentation was exactly,
but I remember that I wasn't quite happy about it.
This presentation is really about silly things that I
do with lots of spreadsheets.

> "Open data"!?
> ==============

For the past few years, I've been hearing a lot about "open data".
I was apparently even an expert in open data even though I didn't
really know what it was. So I started going to various repositories
of open data, downloading all of the data, and looking at what they
contained.

![Plot of dataset counts by site](../socrata-summary/figure/big_portals_datasets.png)

With hardly any effort, I had amassed tens of thousands of datasets.
Here we have a plot of the number of datasets on each of the different repositories I was looking at.
I quickly realized that nobody had any idea what was in these datasets,
so I started making tools to understand what was in them.

> Why nobody knows what's in these datasets
> =========================================

Before I talk about these tools, let me explain why nobody knows what's
in these datasets.

[![Tom holding a letterpress print of a CSV file](../print-formaldehyde/csv-print.jpg)](../print-formaldehyde)

As far as I can tell, a "dataset" is a
"file with lots of numbers in it",
maybe in CSV format or maybe in Microsoft Word format.

![A spreadsheet](bus-spreadsheet.png)

How do we figure out what's in a bunch of spreadsheets today?
We could manually look at every cell in every spreadsheet,
but this would take a long time.

Some people think it would take a bit less long if we have good spreadsheets.

> Open data guidelines
> ======================
> * Open Knowledge Foundation [Open Data Census](http://census.okfn.org/)
> * Tim Berners-Lee [Five Stars](http://inkdroid.org/journal/2010/06/04/the-5-stars-of-open-linked-data/) of open linked data.
> * Open Government Working Group [8 Principles of Open Government Data](http://www.opengovdata.org/home/8principles)
> * Sunlight Foundation [Open Data Policy Guidelines](http://sunlightfoundation.com/opendataguidelines/)
> * Open Data Institute [Certificates](https://certificates.theodi.org/)

Many "open data" guidelines provide direction as to how a person
or organization with lots of datasets might allow other people
to use them. For example, it's a good idea to have your data on the
internet in some form. Even better, it should have a free license,
certain metadata, a standard format, and so on.

These guidelines put a lot of
responsibility on the publishers of data.
This can be a lot of work, so it's unreasonable
for us to expect this to happen for anything other than the most
important of datasets.

> Standard Schemas
> =================
> * RSS
> * Atom
> * [GeoJson](http://geojson.org/)
> * &c.

Many groups have developed standard schemas for particular sorts
of data. And when I say "schema", I loosly mean "a definition of
what the columns should be and what the rows mean".

![](hovering-schema.png)

For example, I'll invent right now a schema for data about research
about how people use toilets.
I asked a bunch of people how they use toilets in different scenarios,
and some of the results are in this spreadsheet. I asked each person
about twelve different scenarios. Each row in the spreadsheet corresponds
to a scenario within a person; there is one row per person.
The "participantId" column defines the person, and the "task", "privacy",
and "cleanliness" columns define the scenario; these four columns thus
act as a unique index on the table. The other columns are about their
responses to the questions. I'll save the full discussion for a talk
about the [viscious hovering cycle](../hovering-cycle); the point here
is that a schema is roughly a definition of what the different columns
and rows mean and what values are allowed in the cells.

![slide about different organizations](standards.png)

A standard schema is nice when different organizations have similar datasets,
like when multiple bloggers are publishing blog posts.
But it still puts a burden on the publishers of data, and
things only become standardized if everyone somehow adopts the "standard".

I've talked about a few ways of figuring out what's in spreadsheets

1. Reading the whole spreadsheet
2. Documenting spreadsheets so it's easier to read them
3. Standardizing formats so it's even easier to read them

Let's talk about one more thing that people do to figure out what's in their
spreadsheets.

> Graphs and stuff
> =================

Lots of people say that graphs make us understand data. So a lot of these
repositories of spreadsheets let people make graphs.

On the French national rail company data site, you can look at tables,

![](sncf-tableau.png)

but you can also make graphs!

![](sncf-analyse.png)

OpenData Lombardia does it too!

![](lombardia-grafico.png)

On any dataset, you click a button to get a table or another button to
get graphs, like magic! Well not really.

These graphs are nice, but we still have the same problem as before:
Even if we have the most perfect graphs about each separate spreadsheet,
we won't have time to look at of the millions of spreadsheets; before
it's worth looking at these graphs, we have to choose a small group of
spreadsheets to make graphs about.

> Looking at lots of spreadsheets
> ----------------------------------
> 1. Reading the whole spreadsheet
> 2. Documenting spreadsheets so it's easier to read them
> 3. Standardizing formats so it's even easier to read them
> 4. Graphs and stuff

I've told you now about four ways that people try to figure out
what's in all of their spreadsheets. I don't think any of them
works very well.

> Word of mouth
> ================

In practice, I find that people learn what's in a spreadsheet through word
of mouth, without looking directly at the spreadsheet. I know what's in the
various datasets published by the United States Census, but I've never used
any of them myself. I know what's in New York City's tax parcel boundaries
dataset (PLUTO) because lots of people have
been [talking](http://spatialityblog.com/2013/04/04/a-modest-proposal-for-nyc-tax-parcel-data/) [about](http://www.codeforamerica.org/blog/2013/07/25/epic-win-for-nycs-open-data-community-pluto-is-free/)
it, but I haven't looked at it myself.
And I know that several cities release data about the use of bike-share programs
because people get excited every time this happens, but again, I haven't looked
at any of it myself.

[![Nicole Neditch](nicole.jpg)](https://twitter.com/nneditch)
[![Noel Hidalgo](noel.jpg)](http://noneck.org/)

The people I meet who know the most about what's in these large collections
of disparate open data spreadsheets are the people in charge of putting them
on the internet in a form that people can use them. Surprisingly to me,
these often aren't the ones who are looking very analytically at the spreadsheets.

[![Chris Metcalf](chris.jpg)](http://chrismetcalf.net/)
[![Tim Wisniewski](tim.jpg)](http://timwis.com/)

People like Nicole, Noel, Chris, and Tim
have worked with various organizations (mostly governments) to publish
their data, and they know a lot the many datasets being published by the
organizations they respectively work. If you want to know whether a city
publishes a particular dataset, these are the people you want to ask.

[![](top-ten-elevator-offenders.png)](http://www.nyc.gov/html/dob/html/safety/top10_elev_offenders.shtml)

Let me tell you just one story. A friend and I came across New York City's
"Top Ten Elevator Offenders" dataset and couldn't stop laughing, mainly
because of the title. We mentioned it to Noel, and he told us about all of
the wonderful politics through which only the top ten, rather than all,
of the elevator offenders were published.

Unfortunately, Noel and these other people
don't have time to serve as everyone's personal
search engine, and there isn't an easy way for them to convey all of this
knowledge to others. And they each only know about the data they've worked
on in their respective governments; they don't know much about other governments.

> What's in all these data
> =============================================================

I've told you why nobody knows what's in her data.
Now I'm going to tell you what's in everybody's data.
Well not really.

> I like recursion
> ==================

Considering that we're talking about open **data**, I figured that we should
be searching through these data in a **data-driven** way.

> I heard you like spreadsheets
> ----------------------------------
> ![](../dataset-as-datapoint/spreadsheet-spreadsheet.png)

I started downloading
lots of datasets and running crude statistics across them.
I wound up with a spreadsheet where each row was a spreadsheet.

(A small note: When I first started doing this, I was only looking at catalog
sites that used the Socrata Open Data Portal software.)

![Plot of dataset counts by site](../socrata-summary/figure/big_portals_datasets.png)

Many organizations report this count of datasets that they publish, and this number
turns out to be nearly useless.

> **Column names**: "grade", "year", "demographic", "number_tested", "mean_scale_score", "num_level_1", "pct_level_1", "num_level_2", "pct_level_2", "num_level_3", "pct_level_3", "num_level_4", "pct_level_4", "num_level_3_and_4", "pct_level_3_and_4"
>
> **Datasets with these column names**:
>
> * [Math Test Results 2006-2012 - Citywide - Gender](https://data.cityofnewyork.us/d/2bh6-qmgg)
> * [Math Test Results 2006-2012 - Citywide - Ethnicity](https://data.cityofnewyork.us/d/vve2-26rs)
> * [English Language Arts (ELA) Test Results 2006-2012 - Citywide - SWD](https://data.cityofnewyork.us/d/d72n-ivax)
> * [English Language Arts (ELA) Test Results 2006-2012 - Citywide - ELL](https://data.cityofnewyork.us/d/72db-huua)
> * [Math Test Results 2006-2012 - Citywide - SWD](https://data.cityofnewyork.us/d/ufu7-zp25)
> * [English Language Arts (ELA) Test Results 2006-2012 - Citywide - All Students](https://data.cityofnewyork.us/d/89di-hi4s)
> * [Math Test Results 2006-2012 - Citywide - ELL](https://data.cityofnewyork.us/d/ngbi-cq85)
> * [English Language Arts (ELA) Test Results 2006-2012 - Citywide - Gender](https://data.cityofnewyork.us/d/cs9m-cz6f)
> * [Math Test Results 2006-2012 - Citywide - All Students](https://data.cityofnewyork.us/d/fxwm-3t4n)
> * [English Language Arts (ELA) Test Results 2006-2012 - Citywide - Ethnicity](https://data.cityofnewyork.us/d/p5w7-g72z)

<!--
[{"id": "2bh6-qmgg", "name": "Math Test Results 2006-2012 - Citywide - Gender"}
, {"id": "vve2-26rs", "name": "Math Test Results 2006-2012 - Citywide - Ethnicity"}
, {"id": "d72n-ivax", "name": "English Language Arts (ELA) Test Results 2006-2012 - Citywide - SWD"}
, {"id": "72db-huua", "name": "English Language Arts (ELA) Test Results 2006-2012 - Citywide - ELL"}
, {"id": "ufu7-zp25", "name": "Math Test Results 2006-2012 - Citywide - SWD"}
, {"id": "89di-hi4s", "name": "English Language Arts (ELA) Test Results 2006-2012 - Citywide - All Students"}
, {"id": "ngbi-cq85", "name": "Math Test Results 2006-2012 - Citywide - ELL"}
, {"id": "cs9m-cz6f", "name": "English Language Arts (ELA) Test Results 2006-2012 - Citywide - Gender"}
, {"id": "fxwm-3t4n", "name": "Math Test Results 2006-2012 - Citywide - All Students"}
, {"id": "p5w7-g72z", "name": "English Language Arts (ELA) Test Results 2006-2012 - Citywide - Ethnicity"}
]
-->

These "datasets" can all be thought of as subsets of the same single dataset.
If I just take different subsets of a single spreadsheet (and optionally
pivot/reshape the subsets), I can easily expand one spreadsheet into over 9000.

So maybe we should look at other things too.
How many times people have downloaded these datasets?

![Plot of dataset counts and download counts by site](../socrata-summary/figure/big_portals_density_text.png)

Here, the x axis is the same as the previous plot but the y axis is the total
downloads by site.

I can also look at how big they are.
It turns out that most of them are pretty small.

<!--
  s <- read.csv('~/t/socrata-analysis/socrata-deduplicated.csv')
  sum(is.na(s$nrow))                                                                                                  
[1] 476
  mean(s$nrow > 100, na.rm = T)                                                                                       
[1] 0.2507369
  mean(s$nrow > 1000, na.rm = T)                                                                                      
[1] 0.1162797
  mean(s$nrow > 10000, na.rm = T)                                                                                     
[1] 0.05230904
  mean(s$nrow > 100000, na.rm = T)                                                                                    
[1] 0.01799659
-->

> Dataset sizes
> ===============
> In Socrata catalogs as of last summer,
> 
> * Only 25% of datasets had more than 100 rows.
> * Only 12% of datasets had more than 1,000 rows.
> * Only 5% of datasets had more than 10,000 rows.

Regardless of the format of these datasets, you can think of them as
spreadsheets without code, where columns are variables and rows are records.
In Socrata catalogs as of last summer,

* Only 25% of datasets had more than 100 rows.
* Only 12% of datasets had more than 1,000 rows.
* Only 5% of datasets had more than 10,000 rows.

Now let's look at something different: The formats of these datasets.

![](../socrata-formats/figure/deduplicated.png)

These "datasets" are mostly available in comma-separated values (CSV) format.
More precisely, you can get them in a lot of formats and CSV is just the first
of them in alphabetical order.

You can actually learn a lot from just looking at file formats.

![](../socrata-formats/figure/sf.png)

Here are the file formats for just the data catalog for the San Francisco government.

![](../socrata-formats/figure/sf-changes.png)

If we look over time, we can see bursts of CSV upload.
Here, the x-axis is time and the y-axis is number of datasets uploaded that month.
The color indicates whether the dataset is in CSV format.

![](../socrata-formats/figure/sf-shapefile.png)

If we change the graph slightly, we see bursts of shapefile upload.
My guess is that someone was going around trying to get departments
to upload their data and hit departments with lots of spreadsheets
and shapefiles at those respective spikes.

![](../socrata-formats/figure/deduplicated.png)

I'm just showing you this file format thing to give you a feel for how very
crude inspection of these datasets can yield interesting information.
Now let's look at another thing!

> Graphs?
> ==========

I said earlier that people like making graphs. They like making graphs so much
that people like Socrata put it in their data catalog software, so you can
make graphs of data inside your browser and save them.

![](../socrata-users/figure/n.views.png)

People with special privileges can make private graphs, but most people can
only make public ones; if you save
the graph you make, it shows up on the public website, so I can see how people
are using this feature and how many graphs they're making.
Socrata says that this "democratizes the data experience" by letting ordinary
"citizens" analyze data.

> Main users
> ==========
> * Socrata employees
> * Socrata clients (like data.gov)
> * A bot
> * Ordinary people making lots of different queries on the same spreadsheet
> * People testing apps that take data from Socrata

To make the long story short: It looks like there are a lot of good reasons
to have this feature, but enabling data analysis by ordinary people isn't
one of them.

> Quantifying compliance with guidelines
> =======================================

My main approach in all of this work has been to acquire a lot of spreadsheets
and run silly things across all of them. At some point, I started focusing this
a bit. People have various ideas about what makes for a good dataset, and these
are codified in the various guidelines that I mentioned before. The guidelines
I reference are all written in English, and none are written in code. I started
coming up with software approaches for assessing compliance with these guidelines.

![](../zombie-links/figure/p_link_types.png)

One common guideline is that data should be on the internet. It turns out that
a lot of data were once on the internet but have since disappeared.
This plot shows us what proportion of datasets from each of many data catalog
sites are dead links. There are nuances in the mortality of datasets, but let's
just say that the cyan and purple are portions of the bars are dead datasets
and everything else is live datasets.

![](../zombie-links/figure/p_prop_links.png)

It turns out that data catalog software relates to this. (What do you know!?)
In the CKAN software, people are encouraged to link to data stored on other
servers, and in the Socrata software, people are encouraged to upload data
to Socrata servers. It is thus pretty hard to have dead datasets on the
Socrata software.

> This shouldn't be interesting
> ==============================

The stuff I've been doing is really basic,
so it kind of scares me that people find any of it interesting.
For the various statistics above, I haven't been doing any notable inspection
of the contents of the spreadsheets.

> (Almost) Nobody knows what's in her data
> =================================================

Nobody knows what's in her data,
so even the simplest of studies are informative.

Having been convinced that people have no idea what's in their spreadsheets,
I...

> It's hard to search for spreadsheets
> =================================================

I noticed that it's very hard to find spreadsheets that are relevant
to a particular analysis unless you already know that the spreadsheet exists.
You can search in a generic web search engine, but the spreadsheets might not
be indexed, and the search might not work that well even if the spreadsheets
are indexed.

![](search-delft.png)

Someone might publish a bunch of spreadsheets on a fancy website.
You can use the search bars on these various websites, but they usually do a
very naive keyword search. Also, these sites tend to be quite slow.

![](../dataset-as-datapoint/spreadsheet-spreadsheet.png)

Having assembled a spreadsheet about my spreadsheets, I was quite able to
search through them. Even though I only had metadata in this spreadsheet,
I could do powerful searches because I could create any metadata I wanted.
Here's an example. In my composition of music from spreadsheets and in my
teaching, I often want a dataset whose schema has particular characteristics.
For example, I might want a dataset with lots of date columns.

    SELECT * FROM datasets WHERE "ncol.date" > 3;

So I collected data about the column types in these various tables and ran
queries like this on my spreadsheet.

![](../data-about-open-data-talk-december-2-2013/unsilo.jpg)

Here's another issue with using website-specific search bars.
When I'm looking for spreadsheets, the publishing organization is unlikely
to be my main concern. For example, if I'm interested in data about the
composition of different pesticides, but I don't really care whether the
data were collected by this city government or by that country government.

In my view, we can have one set of tools/people that focus on making
data available in a crude form and another set for assembling these crude
data into something more relevent to the people who want the data

[![](../openprism/taco.png)](http://openprism.thomaslevine.com)

So I made this site that I call OpenPrism. (It's like Prism, but open!!!)
This is a disgustingly simple site that
forwards your search query to 100 other sites that house spreadsheets.

Let me reiterate my thoughts on searching spreadsheets.

> Why searching for spreadsheets is hard
> ==========================================
>
> * Site-specific search
> * Naive search method

I see two main issues in the common means of searching through spreadsheets.
The first issue is that the search is localized to datasets that are published
or otherwise managed by a particular entity; it's hard to search for
spreadsheets without first identifying a specific publisher or repository.

The second issue is that the search method is quite naive; these websites are
usually running keyword searches.

I started to address the first issue by letting people search for lots of
sites at once. Let's talk now about addressing the second issue

> Guessing a schema
> =======================

I think a lot about rows and columns. When we define tables in relational
databases, we can say reasonably well what each column means, based on
names and types, and what a row means, based on unique indices.
In spreadsheets, we still have column names, but we don't get everything
else.

> Brute-force guessing the observational unit
> ============================================

The unique indices tell us quite a lot; they give us an idea about the
observational unit of the table and what other tables we can nicely
join or union with that table. So I made a package that finds unique
indices in ordinary CSV files.

    pip3 install special_snowflake

It's called "special snowflake", but it needs a better name.

You may have noticed that my only trick is to do stupid stuff with
**all** the things. special snowflake just looks at **all** of the
column combinations and checks whether there are any duplicate values.

If we pass the iris dataset to it, ::

    "Sepal.Length","Sepal.Width","Petal.Length","Petal.Width","Species"
    5.1,3.5,1.4,0.2,"setosa"
    4.9,3,1.4,0.2,"setosa"
    4.7,3.2,1.3,0.2,"setosa"
    4.6,3.1,1.5,0.2,"setosa"
    ...

we get no unique keys ::

    $ snowflake iris.csv 

because no combination of columns uniquely identifies the rows.
Of course, if we add an identifier column, ::

    "Id","Sepal.Length","Sepal.Width","Petal.Length","Petal.Width","Species"
    1,5.1,3.5,1.4,0.2,"setosa"
    2,4.9,3,1.4,0.2,"setosa"
    3,4.7,3.2,1.3,0.2,"setosa"
    4,4.6,3.1,1.5,0.2,"setosa"
    ...

that one gets returned. ::

    $ snowflake iris.csv 
    Id

For a more interesting example, let's look at chickweight.

    "weight","Time","Chick","Diet"
    42,0,"1","1"
    51,2,"1","1"
    59,4,"1","1"
    64,6,"1","1"
    76,8,"1","1"
    ...

I could read the documentation on this dataset and tell you
what its statistical unit is (`?ChickWeight` in R), or I could
just let `special_snowflake` figure it out for me.

    $ snowflake chick.csv 
    Time, Chick

The statistical unit is chicks in time. That is, something was
observed across multiple chick, and multiple observations were
taken from each (well, at least one) chick.

Some spreadsheets are have less obvious identifiers. In this
table of 1219 rows and 33 columns,

    $ wget -O poste.csv http://data.iledefrance.fr/explore/dataset/liste-des-points-de-contact-du-reseau-postal-dile-de-france/download/?format=csv
    $ snowflake poste.csv
    adresse, code_postal
    adresse, localite
    identifiant
    libelle_du_site
    wgs84

we find five functional unique keys. Just by looking at the column names,
I'm gussing that the first two are combinations of parts of the postal address
and that the latter three look are formal identifiers.
And when I do things correctly and look at the
[data dictionary](http://data.iledefrance.fr/api/datasets/1.0/liste-des-points-de-contact-du-reseau-postal-dile-de-france/attachments/laposte_description_champs_pointdecontact_pdf/),
I come to the same interpretation.

This tells me that this dataset is about postal service locations,
with one location per row. It also gives me some ideas as to things that can
act as unique identifiers for postal service locations.

> Brute-force guessing foreign keys
> ===================================

Pretty much everything I've done so far looks at individual spreadsheets.
I want to look at relationships between spreadsheets so that I can guess
which spreadsheets are the same as each other, which can be joined to each
other, which are subsets of each other, &c. Also, I'm going to do this with
brute force because that's all I know how to do.

In [commasearch](https://pypi.python.org/pypi/commasearch), I basically run
**all** of the possible inner joins between a bunch of spreadsheets,
and then I can get a feel of the strengths of the connections between pairs
of spreadsheets.

![](network.png)

Let's trace what happens when we compare one spreadsheet to our database
of spreadsheets.

![](factorial-1.png)

We choose a group of columns in one table and a group of the same number
of columns in the other table.

![](factorial-1-overlap.png)

We look at how much these columns overlap.
Spreadsheets with high overlap might contain foreign key relationships
or might be the same spreadsheet.

![](factorial-2.png)

Then we repeat this for all combinations of columns.

> Search Engine
> ================

It took me a while to figure out how to expose these brute force connections
in a way that people could understand them. Here's what I came up with.

![](wordsearch.png)

When we search for ordinary written documents, we send words into a search
engine and get pages of words back.

![](commasearch.png)

What if we could search for spreadsheets
by sending spreadsheets into a search engine and getting spreadsheets back?
And thus I made comma search.

    pip3 install commasearch                                                                            

Here's how you use it.

    , --index this.csv that.csv and.csv so.csv on.csv                                                   

First, you index a bunch of CSV files. To assist with this, I wrote a separate
thing called "pluplusch" that emits a list of CSV files.

    pluplusch --urls | , --index -

A side note, these "datasets" that pluplusch emits are mostly CSV files, but
it might be worth looking at pluplusch if you want a corpus for spreadsheet research.

Then you search for a CSV file.

    , --verbose http://www-static.bouldercolorado.gov/docs/opendata/2000_Construction_Permits.csv
    {"url": "http://data.ottawa.ca/storage/f/2013-04-16T201942/RingPost_Master_List_-Feb12_2013.csv", "search_columns": [4], "overlap": 4, "result_columns": [10], "nrow": 809}
    {"url": "http://data.ottawa.ca/storage/f/2013-04-16T201942/RingPost_Master_List_-Feb12_2013.csv", "search_columns": [5], "overlap": 0, "result_columns": [10], "nrow": 809}
    {"url": "http://data.ottawa.ca/storage/f/2013-04-16T201942/RingPost_Master_List_-Feb12_2013.csv", "search_columns": [6], "overlap": 0, "result_columns": [10], "nrow": 809}
    {"url": "http://data.ottawa.ca/storage/f/2013-04-16T201942/RingPost_Master_List_-Feb12_2013.csv", "search_columns": [7], "overlap": 0, "result_columns": [10], "nrow": 809}
    {"url": "http://data.ottawa.ca/storage/f/2013-04-16T201942/RingPost_Master_List_-Feb12_2013.csv", "search_columns": [8], "overlap": 1, "result_columns": [10], "nrow": 809}
    {"url": "http://data.ottawa.ca/storage/f/2013-04-16T201942/RingPost_Master_List_-Feb12_2013.csv", "search_columns": [9], "overlap": 5, "result_columns": [10], "nrow": 809}
    {"url": "http://data.ottawa.ca/storage/f/2013-04-16T201942/RingPost_Master_List_-Feb12_2013.csv", "search_columns": [10], "overlap": 138, "result_columns": [10], "nrow": 809}
    {"url": "http://data.ottawa.ca/storage/f/2013-04-16T201942/RingPost_Master_List_-Feb12_2013.csv", "search_columns": [11], "overlap": 132, "result_columns": [10], "nrow": 809}
    {"url": "http://data.ottawa.ca/storage/f/2013-04-16T201942/RingPost_Master_List_-Feb12_2013.csv", "search_columns": [12], "overlap": 556, "result_columns": [10], "nrow": 809}
    
This says that columns 10, 11, and 12 in `2000_Construction_Permits.csv` overlap
a lot with column 10 of `RingPost_Master_List_-Feb12_2013.csv`.

    $ head -n5 2000_Construction_Permits.csv
    Case Number,Address,Assessor ID,Case Status,Category,Building Uses and Work Scopes,Permit Types, Total Project Value , Total Subpermit Value ,Applied,Approved,Issued,CO Date,Completion Date,New Res Unit,Existing Res Unit,Affordable Hsg Unit,New SF,Remodel SF,Narrative Description,Primary First Name,Primary Last Name,Primary Company,Contractor First Name,Contractor Last Name,Contractor Company,Owner1 First Name,Owner1 Last Name,Owner1 Company,Owner2 First Name,Owner2 Last Name,Owner2 Company
    PMT2000-00001,1343 WILDWOOD CT,0066684,REV,Residential,"Single Family Detached Dwelling, Existing",Mechanical, $899.00 , $899.00 ,1/7/2000,1/7/2000,12/17/2099,,,,1,,,,GAS LOG INSERT,,THE GAS CONNECTION,THE GAS CONNECTION,,THE GAS CONNECTION,THE GAS CONNECTION,ALEXANDER,BROWN,,,KATHY VOGELEY,
    PMT2000-00002,263 PEARL ST    8,0101764,N&V,Residential,"Townhomes, Existing",Mechanical," $1,250.00 "," $1,250.00 ",1/7/2000,1/7/2000,12/17/2099,,,,1,,,,GAS LOGS IN FIREPLACE AND GAS STOVE IN KITCHEN,,NORM'S PLUMBING AND HEATING,NORM'S PLUMBING AND HEATING,,NORM'S PLUMBING AND HEATING,NORM'S PLUMBING AND HEATING,ROBERT,JUSTIS,,LESLIE,N,
    PMT2000-00003,3000 WALNUT ST,0065954,CLS,NonResidential,"Commercial/Retail, Existing",Electrical, $600.00 , $600.00 ,1/7/2000,1/7/2000,12/17/2099,,,,,,,,ELECTRICAL WIRING FOR 2 ELECTRIC SIGNS,,"AAA ELECTRIC COMPANY, INC.","AAA ELECTRIC COMPANY, INC.",,"AAA ELECTRIC COMPANY, INC.","AAA ELECTRIC COMPANY, INC.",TERRENCE,OCONNOR,,,MIDAS,
    PMT2000-00004,2515 BROADWAY,0003260,N&V,Residential,"Single Family Detached Dwelling, Existing",Mechanical," $1,950.00 "," $1,950.00 ",1/7/2000,1/7/2000,12/17/2099,,,,1,,,,INSTALL 65000 BTU SPACE HEATER IN LIVING ROOM,,CONTROL SERVICE CENTER INC,CONTROL SERVICE CENTER INC,,CONTROL SERVICE CENTER INC,CONTROL SERVICE CENTER INC,HOSPITAL,COMMUNITY,,,MOCK PROPERTY MANAGEMENT,
    
Columns 10, 11, and 12 are called "CO Date", "Completion Date", "New Res Unit".

    $ head -n5 RingPost_Master_List_-Feb12_2013.csv
    Post_ID,Mid_block_ID,Street_1,Street_2,Street_3,Side,Adjacent_to,latitude,longitude,Core,Notes
    1,1,Laurier Ave E,Henderson Ave,Nelson St,South,210 Laurier Ave E,45.42571,-75.681794,New,
    2,1,Laurier Ave E,Henderson Ave,Nelson St,South,210 Laurier Ave E,45.425816,-75.681558,New,
    3,1,Laurier Ave E,Henderson Ave,Nelson St,South,238 Laurier Ave E,45.425902,-75.681247,New,
    4,2,Laurier Ave E,Cumberland St,Waller St,North,75 Laurier Ave E,45.42408,-75.686239,New,

Column 10 of `RingPost_Master_List_-Feb12_2013.csv` is called "Core".

You get a list of a bunch of statistics about the joins.

The result in this case is a bit underwhelming: The overlap is on
empty strings (""). I could change that.

## Review
Tons of people are releasing lots of data. That's too many fun data,
and I couldn't decide which ones to look at, so I decided to just look
at all of them. It turns out that this is hard.

> Attempts at understanding all these datasets
> =================================================================
> * Write data dictionaries, &c.
> * Agree on standards
> * Make graphs and stuff
> * Ask someone

We don't have great ways of figuring out what's in all these spreadsheets.
People try to to tidy up and document the data really well, but that's
a lot of work. People also try to make it easy for people to make graphs;
I don't think that gets us much further. The most effective thing, in my
experience, is to just ask someone who has already looked at the data.

> My approach: Brute force
> =========================

My only trick is to do stupid things many times.
I've been downloading lots of spreadsheets and doing
crude, silly things on each of them.

![](../zombie-links/figure/p_prop_links.png)

I started out by looking at very simple things like how big they are.
I also tried to quantify other people's ideas of how good datasets are,
like whether they are up-to-date and whether they are freely licensed.

> ???
> ====

The main thing that I keep seeing is that nobody has any idea what's in all
of these spreadsheets; I notice this because people continue to be interested
in results that I find pretty boring.

> "Search"

I eventually started thinking about how to show other people what's
in all of these spreadsheets. I noticed that it's pretty hard to search
for spreadsheets.

![](../data-about-open-data-talk-december-2-2013/unsilo.jpg)

The first issue is that you need to type your search into multiple search
bars. Dedicated search engines like DuckDuckGo don't index all of the
spreadsheets, so you're stuck using site-specific searches, and these only
search within their specific sites.

[![](../openprism/taco.png)](http://openprism.thomaslevine.com)

I wrote OpenPrism to deal with this; you type your search into one search
bar, and it's as if you typed it in to all of the data catalogs I know about
at once.

The other issue is that our methods for searching spreadsheets are quite naive.
Most of these data catalogs look for exact string matches in the datasets or
for something similarly crude.

![](wordsearch.png)

The way we look up information today is to type some words into a search engine
and read the first few results. Why is that not the way we look up data?

![](commasearch.png)

We have designed search engines to look for things arranged in paragraphs
and sections and pages and hyperlinks; we haven't designed search engines for
things with tables and rows and columns and join keys.

> Relevance
> =================

I doubt you're going to use any of the tools I've been talking about,
but let me pass along some practical conclusions of this work.

> ## 1. Nobody understands her own data. (You're not the only one.)

Hardly anyone has a good understanding of what data are available in
her organization. Or if someone thinks she knows about all of the data,
she's deluding herself. So it's okay if you don't know what data you
hahve, and you shouldn't expect someone else to know.

> ## 2. If people aren't documenting their data, don't expect them to document their data

It would be nice if everyone always wrote schemas for her data.
Unfortunately, this doesn't happen. And when it does happen,
the schema might be wrong anyway.

If you're having trouble getting people to document their data,
just accept that they won't, and figure out a way around it.

> ## 3. Brute force can help you understand your data

Computers are cheap. If you're wondering something about your spreadsheets,
open **all** of them up and find out!
